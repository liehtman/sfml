{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model for now: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = RandomForestClassifier(n_estimators=100, class_weight= None, criterion='entropy', max_features='sqrt', random_state=123)\n",
    "\n",
    "[0.99115753 0.9919968  0.99133321 0.99133818 0.9915882 ]\n",
    "avg: 0.9914827862478924\n",
    "\n",
    "Train: 0.9999779010926789\n",
    "Test: 0.9925608234263946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек, назначение стоп-слов и констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian'))\n",
    "KEYED_VECTORS_FILE = 'w2v_allwords_model'\n",
    "WORD2VEC_FILE = \"word2vec_all_words.model\"\n",
    "embedding_dim = 100\n",
    "# w2v = Word2Vec(min_count=1, size=embedding_dim, workers=4)\n",
    "# w2v.save(WORD2VEC_FILE)\n",
    "# del w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для предобрабки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    # Избавление от html-тегов\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def preproc_data(df_input):\n",
    "    # Избавление от html-тегов\n",
    "    df_output = df_input.copy()\n",
    "    df_output['name'] = df_output['name'].map(striphtml)\n",
    "    df_output['description'] = df_output['description'].map(striphtml)\n",
    "    \n",
    "    return df_output\n",
    "        \n",
    "def prepare_w2v(path, update):\n",
    "    # Обучение и дообучение word2vec\n",
    "    def iterate_rows(df):\n",
    "        print('loading model')\n",
    "        model = Word2Vec.load(WORD2VEC_FILE)\n",
    "        sentences = []\n",
    "        for i, row in df.iterrows():\n",
    "            if i % 10000 == 0:\n",
    "                print(\"Currently on row: {}; Currently iterrated {}% of rows\".format(i, (i + 1)/len(df.index) * 100))\n",
    "\n",
    "            words_list = re.findall('\\w+', row['name'])\n",
    "            sentences.append([word.lower() for word in words_list if word.lower() not in stops])\n",
    "            words_list = re.findall('\\w+', row['description'])\n",
    "            sentences.append([word.lower() for word in words_list if word.lower() not in stops])\n",
    "    \n",
    "        print('building vocab. update = ', update)\n",
    "        model.build_vocab(sentences, update=update)    \n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        \n",
    "        del sentences\n",
    "        model.save(WORD2VEC_FILE)\n",
    "        word_vectors = model.wv\n",
    "        word_vectors.save(KEYED_VECTORS_FILE)\n",
    "        print('\\n')\n",
    "        print('Vocab:', len(word_vectors.vocab))\n",
    "        print('='*10)\n",
    "        del model\n",
    "        del word_vectors\n",
    "           \n",
    "    if path == 'other.csv':\n",
    "#         594533\n",
    "        df_1 = pd.read_csv(path, '\\t', names=['name', 'description'], header=0, nrows=200000)\n",
    "        iterate_rows(df_1)\n",
    "        del df_1\n",
    "        df_1 = pd.read_csv(path, '\\t', names=['name', 'description'], header=0, skiprows=200000, nrows=200000)\n",
    "        iterate_rows(df_1)\n",
    "        del df_1\n",
    "        df_1 = pd.read_csv(path, '\\t', names=['name', 'description'], header=0, skiprows=400000)\n",
    "        iterate_rows(df_1)\n",
    "        del df_1\n",
    "    else:\n",
    "        df_1 = pd.read_csv(path, '\\t', usecols=['name', 'description'])\n",
    "        iterate_rows(df_1)\n",
    "        del df_1\n",
    "\n",
    "def get_avg_vector(sentence):\n",
    "    # Получение распределенного представления предложения путем сложения векторов слов\n",
    "    res = sum([w2v[word.lower()] for word in re.findall('\\w+', sentence) if word.lower() not in stops and word.lower() in w2v.vocab.keys()])\n",
    "    if isinstance(res, int):\n",
    "        res = np.array([0]*embedding_dim)\n",
    "    return res / len(res)\n",
    "\n",
    "def create_vectorized_df(df_input):\n",
    "    # Преобразование очищенного датафрейма в векторизованный\n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    df_output['name'] = df_output['name'].map(get_avg_vector)\n",
    "    df_output['description'] = df_output['description'].map(get_avg_vector)\n",
    "\n",
    "    zipped = list(zip(*df_output['name']))\n",
    "    for i in range(embedding_dim):\n",
    "        df_output['name_x%s'%(i)] = zipped[i]\n",
    "        \n",
    "    zipped = list(zip(*df_output['description']))\n",
    "    for i in range(embedding_dim):\n",
    "        df_output['description_x%s'%(i)] = zipped[i]\n",
    "    \n",
    "    df_output.drop(['id', 'name', 'description'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение word2vec и полная предобрабка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', '\\t')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.pipe(preproc_data)\n",
    "df.head()\n",
    "print(df_train.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train.csv', 'test.csv', 'other.csv']\n",
    "for i, path in enumerate(datasets):\n",
    "    if i == 0: prepare_w2v(path, update=False)\n",
    "    else: prepare_w2v(path, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = KeyedVectors.load(KEYED_VECTORS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = df.pipe(create_vectorized_df)\n",
    "df_vectorized.info()\n",
    "df_vectorized.to_csv('vectorized_w2v_allwords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = pd.read_csv('vectorized_w2v_allwords.csv')\n",
    "df_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vectorized.shape)\n",
    "y = df_vectorized['target']\n",
    "X = df_vectorized.drop([\"target\"], axis=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.externals.joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск лучшего леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'class_weight':['balanced', None],\n",
    "    'criterion':['entropy', 'gini'],\n",
    "    'max_features':[None, 'log2', 'sqrt', ] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, verbose=100, cv=cv, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "predict = model.predict_proba(X_train)\n",
    "score = roc_auc_score(y_train, predict[:, 1])\n",
    "print('Train:', score)\n",
    "predict = model.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, predict[:, 1])\n",
    "print('Test:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, class_weight= None, criterion='entropy', max_features='sqrt', random_state=123)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(X_train)\n",
    "score = roc_auc_score(y_train, predict[:, 1])\n",
    "print('Train:', score)\n",
    "predict = model.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, predict[:, 1])\n",
    "print('Test:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv', '\\t')\n",
    "df_test.head()\n",
    "\n",
    "df = df_test.pipe(preproc_data)\n",
    "print(df_test.shape)\n",
    "print(df.shape)\n",
    "\n",
    "df_vectorized = df.pipe(create_vectorized_df)\n",
    "df_vectorized.info()\n",
    "df_vectorized.to_csv('vectorized_test_allwords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = pd.read_csv('vectorized_test_allwords.csv')\n",
    "df_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vectorized.as_matrix()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, class_weight= None, criterion='entropy', max_features='sqrt', random_state=123)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.read_csv('sampleSubmission.csv', ',')\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(X)[:,1]\n",
    "subs['target'] = predict\n",
    "subs.to_csv('subs_allwords.csv', sep =',', index=False)\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S\n",
    "Так же, был другой вариант предобработки: Средние вектора word2vec, умноженные на tf-idf скоры соответствуюших слов. Получилось чуть хуже, обидненько."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
